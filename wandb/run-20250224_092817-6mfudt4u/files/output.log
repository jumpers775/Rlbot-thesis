Created new wandb run! 6mfudt4u
Learner successfully initialized!
Press (p) to pause (c) to checkpoint, (q) to checkpoint and quit (after next iteration)

--------BEGIN ITERATION REPORT--------
Policy Reward: 0.02945
Policy Entropy: 4.49871
Value Function Loss: nan

Mean KL Divergence: 0.00000
SB3 Clip Fraction: 0.00000
Policy Update Magnitude: 0.02696
Value Function Update Magnitude: 0.02808

Collected Steps per Second: 28,495.08956
Overall Steps per Second: 17,376.63201

Timestep Collection Time: 1.75497
Timestep Consumption Time: 1.12292
PPO Batch Consumption Time: 0.53879
Total Iteration Time: 2.87789

Cumulative Model Updates: 1
Cumulative Timesteps: 50,008

Timesteps Collected: 50,008
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 0.17088
Policy Entropy: 4.49874
Value Function Loss: 0.06967

Mean KL Divergence: 0.00000
SB3 Clip Fraction: 0.00000
Policy Update Magnitude: 0.03666
Value Function Update Magnitude: 0.03627

Collected Steps per Second: 29,693.56319
Overall Steps per Second: 15,259.34831

Timestep Collection Time: 1.68400
Timestep Consumption Time: 1.59294
PPO Batch Consumption Time: 0.53132
Total Iteration Time: 3.27694

Cumulative Model Updates: 3
Cumulative Timesteps: 100,012

Timesteps Collected: 50,004
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 0.43487
Policy Entropy: 4.49876
Value Function Loss: 0.12872

Mean KL Divergence: 0.00000
SB3 Clip Fraction: 0.00000
Policy Update Magnitude: 0.04612
Value Function Update Magnitude: 0.05786

Collected Steps per Second: 23,067.07910
Overall Steps per Second: 10,179.68135

Timestep Collection Time: 2.16776
Timestep Consumption Time: 2.74437
PPO Batch Consumption Time: 0.68428
Total Iteration Time: 4.91214

Cumulative Model Updates: 6
Cumulative Timesteps: 150,016

Timesteps Collected: 50,004
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 0.13317
Policy Entropy: 4.49877
Value Function Loss: 0.13085

Mean KL Divergence: 0.00001
SB3 Clip Fraction: 0.00000
Policy Update Magnitude: 0.04664
Value Function Update Magnitude: 0.06344

Collected Steps per Second: 15,311.98510
Overall Steps per Second: 8,091.98793

Timestep Collection Time: 3.26620
Timestep Consumption Time: 2.91423
PPO Batch Consumption Time: 0.72000
Total Iteration Time: 6.18043

Cumulative Model Updates: 9
Cumulative Timesteps: 200,028

Timesteps Collected: 50,012
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 0.01211
Policy Entropy: 4.49876
Value Function Loss: 0.08583

Mean KL Divergence: 0.00001
SB3 Clip Fraction: 0.00000
Policy Update Magnitude: 0.04389
Value Function Update Magnitude: 0.06234

Collected Steps per Second: 15,132.65129
Overall Steps per Second: 7,369.92270

Timestep Collection Time: 3.30438
Timestep Consumption Time: 3.48050
PPO Batch Consumption Time: 0.87454
Total Iteration Time: 6.78487

Cumulative Model Updates: 12
Cumulative Timesteps: 250,032

Timesteps Collected: 50,004
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: -0.24099
Policy Entropy: 4.49875
Value Function Loss: 0.00724

Mean KL Divergence: 0.00001
SB3 Clip Fraction: 0.00000
Policy Update Magnitude: 0.03607
Value Function Update Magnitude: 0.05306

Collected Steps per Second: 14,490.21031
Overall Steps per Second: 7,486.40735

Timestep Collection Time: 3.45061
Timestep Consumption Time: 3.22817
PPO Batch Consumption Time: 0.79860
Total Iteration Time: 6.67877

Cumulative Model Updates: 15
Cumulative Timesteps: 300,032

Timesteps Collected: 50,000
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: -0.14357
Policy Entropy: 4.49874
Value Function Loss: 0.00801

Mean KL Divergence: 0.00001
SB3 Clip Fraction: 0.00000
Policy Update Magnitude: 0.02861
Value Function Update Magnitude: 0.04503

Collected Steps per Second: 11,290.58967
Overall Steps per Second: 6,871.02035

Timestep Collection Time: 4.42847
Timestep Consumption Time: 2.84847
PPO Batch Consumption Time: 0.69855
Total Iteration Time: 7.27694

Cumulative Model Updates: 18
Cumulative Timesteps: 350,032

Timesteps Collected: 50,000
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 0.00060
Policy Entropy: 4.49874
Value Function Loss: 0.00671

Mean KL Divergence: 0.00000
SB3 Clip Fraction: 0.00000
Policy Update Magnitude: 0.02435
Value Function Update Magnitude: 0.04081

Collected Steps per Second: 12,069.18980
Overall Steps per Second: 7,005.13222

Timestep Collection Time: 4.14344
Timestep Consumption Time: 2.99532
PPO Batch Consumption Time: 0.72143
Total Iteration Time: 7.13877

Cumulative Model Updates: 21
Cumulative Timesteps: 400,040

Timesteps Collected: 50,008
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: -0.00624
Policy Entropy: 4.49873
Value Function Loss: 0.01081

Mean KL Divergence: 0.00000
SB3 Clip Fraction: 0.00000
Policy Update Magnitude: 0.02187
Value Function Update Magnitude: 0.03770

Collected Steps per Second: 15,647.29505
Overall Steps per Second: 8,029.78687

Timestep Collection Time: 3.19595
Timestep Consumption Time: 3.03186
PPO Batch Consumption Time: 0.75813
Total Iteration Time: 6.22781

Cumulative Model Updates: 24
Cumulative Timesteps: 450,048

Timesteps Collected: 50,008
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 0.01231
Policy Entropy: 4.49873
Value Function Loss: 0.00993

Mean KL Divergence: 0.00000
SB3 Clip Fraction: 0.00000
Policy Update Magnitude: 0.02099
Value Function Update Magnitude: 0.03619

Collected Steps per Second: 11,380.21477
Overall Steps per Second: 6,745.22563

Timestep Collection Time: 4.39429
Timestep Consumption Time: 3.01954
PPO Batch Consumption Time: 0.72485
Total Iteration Time: 7.41384

Cumulative Model Updates: 27
Cumulative Timesteps: 500,056

Timesteps Collected: 50,008
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 0.01981
Policy Entropy: 4.49873
Value Function Loss: 0.01279

Mean KL Divergence: 0.00000
SB3 Clip Fraction: 0.00000
Policy Update Magnitude: 0.02199
Value Function Update Magnitude: 0.03629

Collected Steps per Second: 16,124.08342
Overall Steps per Second: 8,047.99027

Timestep Collection Time: 3.10095
Timestep Consumption Time: 3.11178
PPO Batch Consumption Time: 0.73955
Total Iteration Time: 6.21273

Cumulative Model Updates: 30
Cumulative Timesteps: 550,056

Timesteps Collected: 50,000
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 0.36850
Policy Entropy: 4.49874
Value Function Loss: 0.00901

Mean KL Divergence: 0.00000
SB3 Clip Fraction: 0.00000
Policy Update Magnitude: 0.02138
Value Function Update Magnitude: 0.03360

Collected Steps per Second: 15,764.21568
Overall Steps per Second: 8,174.62353

Timestep Collection Time: 3.17174
Timestep Consumption Time: 2.94475
PPO Batch Consumption Time: 0.73691
Total Iteration Time: 6.11649

Cumulative Model Updates: 33
Cumulative Timesteps: 600,056

Timesteps Collected: 50,000
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 0.01453
Policy Entropy: 4.49875
Value Function Loss: 0.01317

Mean KL Divergence: 0.00000
SB3 Clip Fraction: 0.00000
Policy Update Magnitude: 0.02071
Value Function Update Magnitude: 0.03206

Collected Steps per Second: 15,710.78000
Overall Steps per Second: 8,238.42801

Timestep Collection Time: 3.18355
Timestep Consumption Time: 2.88751
PPO Batch Consumption Time: 0.67499
Total Iteration Time: 6.07106

Cumulative Model Updates: 36
Cumulative Timesteps: 650,072

Timesteps Collected: 50,016
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: -0.08810
Policy Entropy: 4.49875
Value Function Loss: 0.01472

Mean KL Divergence: 0.00000
SB3 Clip Fraction: 0.00000
Policy Update Magnitude: 0.02149
Value Function Update Magnitude: 0.03305

Collected Steps per Second: 18,344.50672
Overall Steps per Second: 9,534.13620

Timestep Collection Time: 2.72561
Timestep Consumption Time: 2.51870
PPO Batch Consumption Time: 0.60863
Total Iteration Time: 5.24431

Cumulative Model Updates: 39
Cumulative Timesteps: 700,072

Timesteps Collected: 50,000
--------END ITERATION REPORT--------
